{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "Med_QLoRA_Unsloth_full.ipynb"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 🏥 Medical Fine-tuning with QLoRA using Unsloth (Colab-ready)\n",
        "\n",
        "**Educational demo — not for clinical use.**  \n",
        "Ensure you use de-identified data and follow licenses and institutional approvals."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# GPU check\n",
        "import sys, torch, os\n",
        "print(\"Python:\", sys.version)\n",
        "print(\"Torch CUDA available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    try:\n",
        "        print(\"GPU:\", torch.cuda.get_device_name(0))\n",
        "    except:\n",
        "        pass\n",
        "!nvidia-smi || echo \"nvidia-smi not available\""
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Install required libraries\n",
        "This installs Unsloth and common fine-tuning tooling. Runtime may take a minute."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%bash\n",
        "python -V\n",
        "pip install -q --upgrade pip\n",
        "pip install -q unsloth \"transformers>=4.43\" \"datasets>=2.18\" \"accelerate>=0.30\" \"trl>=0.9.6\" bitsandbytes peft\n",
        "python -c \"import unsloth, transformers; print('unsloth', getattr(unsloth, '__version__', 'unknown'))\""
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) (Optional) Hugging Face login\n",
        "If your base model is gated (or you want to push the adapter), provide an HF token."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from getpass import getpass\n",
        "import os\n",
        "HF_TOKEN = os.environ.get(\"HF_TOKEN\") or \"\"\n",
        "if not HF_TOKEN:\n",
        "    HF_TOKEN = getpass(\"Hugging Face token (press Enter to skip): \")\n",
        "if HF_TOKEN:\n",
        "    from huggingface_hub import login\n",
        "    login(HF_TOKEN)\n",
        "    print(\"Logged into Hugging Face\")\n",
        "else:\n",
        "    print(\"Skipping HF login\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Config: choose base model, dataset and hyperparameters\n",
        "Adjust these values to match your GPU availability."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Configuration - adjust as needed\n",
        "BASE_MODEL   = \"meta-llama/Llama-3.1-8B-Instruct\"  # example; change if desired\n",
        "DATASET_NAME = \"bigbio/med_qa\"                     # example domain dataset\n",
        "SPLIT_TRAIN  = \"train\"\n",
        "SPLIT_EVAL   = \"validation\"                       # may be \"validation\" or \"test\"\n",
        "MAX_SEQ_LEN  = 2048\n",
        "MICRO_BATCH  = 1\n",
        "GRAD_ACCUM   = 8\n",
        "EPOCHS       = 1\n",
        "LR           = 2e-4\n",
        "LORA_R       = 32\n",
        "LORA_ALPHA   = 16\n",
        "LORA_DROPOUT = 0.05\n",
        "PROJECT_NAME = \"med-qlora-unsloth\"\n",
        "print(\"Config loaded.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Load & preprocess dataset\n",
        "We convert dataset rows into a `messages`-style chat object: `system -> user -> assistant`.\n",
        "Start with a small subset (e.g., `.select(range(200))`) for debugging if needed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from datasets import load_dataset, DatasetDict\n",
        "import random, textwrap\n",
        "\n",
        "raw = load_dataset(DATASET_NAME)\n",
        "print(raw)\n",
        "\n",
        "def standardize(example):\n",
        "    # Basic heuristics for common medical QA datasets - adjust for your dataset schema.\n",
        "    q = example.get(\"question\") or example.get(\"questions\") or example.get(\"query\") or \"\"\n",
        "    a = example.get(\"answer\") or example.get(\"final_answer\") or example.get(\"long_answer\") or \"\"\n",
        "    if isinstance(a, list):\n",
        "        a = a[0] if a else \"\"\n",
        "    if a is None:\n",
        "        a = \"\"\n",
        "    messages = [\n",
        "        {\"role\":\"system\",\"content\":\"You are a helpful, careful medical assistant. Do not provide professional diagnosis; provide general information and safety guidance.\"},\n",
        "        {\"role\":\"user\",\"content\": str(q)},\n",
        "        {\"role\":\"assistant\",\"content\": str(a)}\n",
        "    ]\n",
        "    return {\"messages\": messages, \"prompt\": str(q), \"answer\": str(a)}\n",
        "\n",
        "# Map dataset - recommend testing on a small subset first for speed\n",
        "proc = {}\n",
        "for split, ds in raw.items():\n",
        "    try:\n",
        "        # small debug: ds = ds.select(range(min(200, len(ds))))\n",
        "        proc[split] = ds.map(standardize, remove_columns=ds.column_names)\n",
        "    except Exception as e:\n",
        "        # fallback: simple mapping to string for unknown schema\n",
        "        proc[split] = ds.map(lambda ex: {\"messages\":[{\"role\":\"user\",\"content\": str(ex)}]}, remove_columns=ds.column_names)\n",
        "\n",
        "dataset = DatasetDict(proc)\n",
        "print(dataset)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Load 4-bit base model & attach QLoRA adapters using Unsloth\n",
        "This uses Unsloth helpers to load the model in 4-bit (NF4 / bnb) and attach PEFT adapters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from unsloth import FastLanguageModel\n",
        "from transformers import AutoTokenizer\n",
        "import torch\n",
        "\n",
        "print(\"Loading tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(\"Loading model in 4-bit (this may take a while)...\")\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = BASE_MODEL,\n",
        "    max_seq_length = MAX_SEQ_LEN,\n",
        "    load_in_4bit = True,\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    use_cache = False,\n",
        "    tokenizer = tokenizer,\n",
        ")\n",
        "\n",
        "# Prepare model for training\n",
        "FastLanguageModel.for_training(model)\n",
        "\n",
        "# Attach LoRA adapters\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = LORA_R,\n",
        "    target_modules = \"all-linear\",\n",
        "    lora_alpha = LORA_ALPHA,\n",
        "    lora_dropout = LORA_DROPOUT,\n",
        "    bias = \"none\",\n",
        ")\n",
        "model.print_trainable_parameters()\n",
        "print('Model ready.')"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Tokenize / apply chat template\n",
        "Use the tokenizer's chat template helper if available to format inputs for supervised finetuning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def apply_template(example):\n",
        "    # Unsloth tokenizers often include an apply_chat_template helper. Use tokenize=False to get text.\n",
        "    try:\n",
        "        text = tokenizer.apply_chat_template(example[\"messages\"], tokenize=False, add_generation_prompt=False)\n",
        "    except Exception:\n",
        "        # fallback: simple concatenation\n",
        "        parts = []\n",
        "        for m in example.get(\"messages\", []):\n",
        "            parts.append(f\"{m.get('role', '')}: {m.get('content', '')}\")\n",
        "        text = \"\\n\".join(parts)\n",
        "    return {\"text\": text}\n",
        "\n",
        "tokenized_ds = {}\n",
        "for split, ds in dataset.items():\n",
        "    # small debug subset option: ds = ds.select(range(200))\n",
        "    tokenized = ds.map(apply_template, remove_columns=ds.column_names)\n",
        "    def tok(ex):\n",
        "        out = tokenizer(ex[\"text\"], truncation=True, padding=\"max_length\", max_length=MAX_SEQ_LEN)\n",
        "        out[\"labels\"] = out[\"input_ids\"].copy()\n",
        "        return out\n",
        "    tokenized = tokenized.map(tok, remove_columns=tokenized.column_names)\n",
        "    tokenized_ds[split] = tokenized\n",
        "\n",
        "from datasets import DatasetDict\n",
        "tokenized = DatasetDict(tokenized_ds)\n",
        "print(\"Tokenized dataset keys:\", list(tokenized.keys()))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) Train with TRL's `SFTTrainer`\n",
        "Start with small epochs/subset for debugging. Use packing for efficiency."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from trl import SFTTrainer, SFTConfig\n",
        "import torch\n",
        "\n",
        "train_data = tokenized.get(SPLIT_TRAIN, tokenized[list(tokenized.keys())[0]])\n",
        "eval_data = tokenized.get(SPLIT_EVAL, None)\n",
        "\n",
        "args = SFTConfig(\n",
        "    output_dir = f\"outputs/{PROJECT_NAME}\",\n",
        "    num_train_epochs = EPOCHS,\n",
        "    per_device_train_batch_size = MICRO_BATCH,\n",
        "    gradient_accumulation_steps = GRAD_ACCUM,\n",
        "    learning_rate = LR,\n",
        "    logging_steps = 10,\n",
        "    save_strategy = \"epoch\",\n",
        "    evaluation_strategy = \"no\" if eval_data is None else \"epoch\",\n",
        "    bf16 = torch.cuda.is_available(),\n",
        "    fp16 = False,\n",
        "    optim = \"paged_adamw_8bit\",\n",
        "    lr_scheduler_type = \"cosine\",\n",
        "    warmup_ratio = 0.05,\n",
        "    report_to = \"none\",\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = train_data,\n",
        "    eval_dataset = eval_data,\n",
        "    dataset_text_field = \"text\",\n",
        "    args = args,\n",
        "    packing = True,\n",
        "    max_seq_length = MAX_SEQ_LEN,\n",
        ")\n",
        "\n",
        "print(\"Beginning training...\")\n",
        "trainer.train()\n",
        "print(\"Training finished.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) Monitor VRAM & performance\n",
        "Use this cell to inspect GPU memory during/after training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch, gc\n",
        "gc.collect()\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"Allocated (MB):\", torch.cuda.memory_allocated()/1e6)\n",
        "else:\n",
        "    print(\"No CUDA available.\")\n",
        "!nvidia-smi || echo \"nvidia-smi not available\""
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9) Save LoRA adapter (small artifact)\n",
        "Saves adapter weights and tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "save_dir = f\"outputs/{PROJECT_NAME}-adapter\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "model.save_pretrained(save_dir)\n",
        "tokenizer.save_pretrained(save_dir)\n",
        "print(\"Adapter saved to:\", save_dir)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10) Quick test / inference\n",
        "Loads the adapter in runtime and generates a sample response for verification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "prompt = \"A 65-year-old with chest pain and shortness of breath: what immediate red flags suggest ER referral?\"\n",
        "messages = [\n",
        "    {\"role\":\"system\",\"content\":\"You are a careful, non-diagnostic medical assistant. Provide safety guidance and recommend clinician review.\"},\n",
        "    {\"role\":\"user\",\"content\": prompt}\n",
        "]\n",
        "# build input text. Use helper if available.\n",
        "try:\n",
        "    input_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "except Exception:\n",
        "    parts = []\n",
        "    for m in messages:\n",
        "        parts.append(f\"{m['role']}: {m['content']}\")\n",
        "    input_text = \"\\n\".join(parts)\n",
        "\n",
        "inputs = tokenizer([input_text], return_tensors=\"pt\").to(model.device)\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    out = model.generate(**inputs, max_new_tokens=256, do_sample=True, temperature=0.7)\n",
        "print(tokenizer.decode(out[0], skip_special_tokens=True))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11) (Optional) Merge adapter and export for Ollama (GGUF)\n",
        "This step can be memory-heavy: re-load base model in 16-bit, merge LoRA, then export to GGUF.\n",
        "Enable only if you have enough RAM/GPU and want a GGUF file for Ollama."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "MERGE_AND_EXPORT = False  # set True to run export steps\n",
        "if MERGE_AND_EXPORT:\n",
        "    from unsloth import FastLanguageModel\n",
        "    from unsloth.export import save_to_gguf, save_to_ollama\n",
        "    print(\"Re-loading base in 16-bit to merge...\")\n",
        "    base16, _ = FastLanguageModel.from_pretrained(\n",
        "        model_name = BASE_MODEL,\n",
        "        max_seq_length = MAX_SEQ_LEN,\n",
        "        load_in_4bit = False,\n",
        "        dtype = None,\n",
        "        tokenizer = tokenizer,\n",
        "    )\n",
        "    merged = FastLanguageModel.merge_lora(base16, model)\n",
        "    os.makedirs('gguf', exist_ok=True)\n",
        "    save_to_gguf(merged, tokenizer, \"gguf/med_qlora\", quantizations=[\"Q8_0\"])\n",
        "    print(\"GGUF export complete.\")\n",
        "    save_to_ollama(\"gguf/med_qlora.Q8_0.gguf\", model_name=\"med-qlora-llama31-8b\")\n",
        "    print(\"Ollama modelfile created.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### Tips\n",
        "- Start with a **small subset** to validate tokenization and generation before full training.\n",
        "- If you hit OOM, reduce `MAX_SEQ_LEN`, use smaller `MICRO_BATCH`, or enable more gradient accumulation.\n",
        "- Always evaluate with clinicians before any real-world use."
      ]
    }
  ]
}