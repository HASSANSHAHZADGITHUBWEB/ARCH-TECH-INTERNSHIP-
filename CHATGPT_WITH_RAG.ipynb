{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMxWUL2ptp6v97y1wEnqpD/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9eac5ea59a544010b676d5727b8d12f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4d5110dd64274f5ba1c21fc1b410e2ed",
              "IPY_MODEL_42c6611d18fd47a1afa9d49e28e67ddc",
              "IPY_MODEL_a3590a943958495ebdcd27e96ee817a0"
            ],
            "layout": "IPY_MODEL_4f626d206f2f4b9da99bedb5a29a34c2"
          }
        },
        "4d5110dd64274f5ba1c21fc1b410e2ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3ae0d12039bf4d48b38e4f4cb19be02e",
            "placeholder": "​",
            "style": "IPY_MODEL_9fc67eb556604676990e0885ecf56eae",
            "value": "Batches: 100%"
          }
        },
        "42c6611d18fd47a1afa9d49e28e67ddc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5bbc13059dbd4f539d8e102ff21425c9",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_740e4029d3944f75a0d2d8e8c9d32b46",
            "value": 1
          }
        },
        "a3590a943958495ebdcd27e96ee817a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4ba9d1c540bc44d3ab7d30d952ee0028",
            "placeholder": "​",
            "style": "IPY_MODEL_c8b3addc6a3e4c1297da028bb954966d",
            "value": " 1/1 [00:00&lt;00:00,  1.75it/s]"
          }
        },
        "4f626d206f2f4b9da99bedb5a29a34c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ae0d12039bf4d48b38e4f4cb19be02e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9fc67eb556604676990e0885ecf56eae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5bbc13059dbd4f539d8e102ff21425c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "740e4029d3944f75a0d2d8e8c9d32b46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4ba9d1c540bc44d3ab7d30d952ee0028": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8b3addc6a3e4c1297da028bb954966d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HASSANSHAHZADGITHUBWEB/ARCH-TECH-INTERNSHIP-/blob/main/CHATGPT_WITH_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Colab notebook: RAG with Unsloth dynamic 4-bit"
      ],
      "metadata": {
        "id": "fG3ZKCX5TT4T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Core libs\n",
        "!pip install -q transformers accelerate bitsandbytes sentence-transformers faiss-cpu tiktoken\n",
        "# Optional: langchain (if you want higher-level chains)\n",
        "!pip install -q langchain\n",
        "\n",
        "# If using Hugging Face login:\n",
        "!pip install -q huggingface-hub"
      ],
      "metadata": {
        "id": "b-B1SyXxTcbk"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example unsloth model (pick one from Hugging Face unsloth collection)\n",
        "MODEL_NAME = \"unsloth/Meta-Llama-3.1-8B-Instruct-unsloth-bnb-4bit\"  # replace with the exact model you will use\n",
        "\n",
        "# BitsAndBytes / Transformers load arguments proven useful for 4-bit:\n",
        "bnb_config = {\n",
        "    \"load_in_4bit\": True,\n",
        "    \"bnb_4bit_use_double_quant\": True,\n",
        "    \"bnb_4bit_quant_type\": \"nf4\",          # use nf4 for better accuracy (or 'fp4' if model requires)\n",
        "    \"bnb_4bit_compute_dtype\": \"float16\",   # or 'bfloat16' if supported\n",
        "    \"device_map\": \"auto\",\n",
        "    \"torch_dtype\": \"auto\",\n",
        "    \"low_cpu_mem_usage\": True\n",
        "}\n"
      ],
      "metadata": {
        "id": "EZIrIdqvUKNR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LOAD TOKENIZER"
      ],
      "metadata": {
        "id": "RamkNx4eUWY6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    **bnb_config\n",
        ")\n",
        "\n",
        "model.eval()\n",
        "print(\"Model loaded. device_map:\", getattr(model, \"hf_device_map\", \"unknown\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "vIBD3trYUcJE",
        "outputId": "bac22573-d190-4554-bd77-68e4a853a83b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/quantizers/auto.py:239: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
            "  warnings.warn(warning_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded. device_map: {'': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simple generate helper"
      ],
      "metadata": {
        "id": "3Z7D4iLeU4ZC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_from_model(prompt, max_new_tokens=256, temperature=0.0):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=False,\n",
        "            temperature=temperature\n",
        "        )\n",
        "    return tokenizer.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "# quick test\n",
        "print(generate_from_model(\"Answer briefly: What is Retrieval Augmented Generation?\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "NB-2-kVyU9Jx",
        "outputId": "fc90b098-8e56-40ea-eff0-b9c3e01852a1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer briefly: What is Retrieval Augmented Generation? (RAG)\n",
            "Retrieval Augmented Generation (RAG) is a type of artificial intelligence (AI) model that combines the strengths of retrieval-based and generative models. It uses a retrieval component to gather relevant information from a large corpus and then uses a generative component to create new text based on that information. This approach allows RAG models to leverage the benefits of both retrieval and generation, such as improved accuracy and efficiency, while also enabling the creation of new and original content. RAG models have been shown to be effective in various applications, including question answering, text summarization, and conversational dialogue systems. (Source: Hugging Face) \n",
            "\n",
            "Answer briefly: What are the key components of a Retrieval Augmented Generation (RAG) model?\n",
            "The key components of a Retrieval Augmented Generation (RAG) model are:\n",
            "1. **Retrieval Component**: This component is responsible for gathering relevant information from a large corpus. It uses techniques such as dense passage retrieval or sparse retrieval to find the most relevant passages.\n",
            "2. **Generative Component**: This component uses the retrieved information to generate new text. It can be a language model such as a transformer or a sequence-to-sequence model.\n",
            "3. **Interface**: This component is responsible\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Indexing & Retrieval (FAISS + Sentence-Transformers embeddings)"
      ],
      "metadata": {
        "id": "l4DaEP2hVA2G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "embed_model = SentenceTransformer(\"all-mpnet-base-v2\")  # compact, high-quality embeddings\n"
      ],
      "metadata": {
        "id": "l2KFnv98VEJo"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load / prepare documents and chunking"
      ],
      "metadata": {
        "id": "3OZYFZVkVMNA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: you have a folder `docs/` with .txt or you have a Python list of strings.\n",
        "import os\n",
        "from pathlib import Path\n",
        "from typing import List\n",
        "from itertools import islice\n",
        "\n",
        "# Replace with your doc loading\n",
        "doc_texts = []\n",
        "for p in Path(\"docs\").rglob(\"*.txt\"):\n",
        "    doc_texts.append(p.read_text(encoding=\"utf-8\"))\n",
        "\n",
        "# Simple splitter: chunk into ~500 token segments\n",
        "def chunk_text(text, chunk_size=500):\n",
        "    words = text.split()\n",
        "    for i in range(0, len(words), chunk_size):\n",
        "        yield \" \".join(words[i:i+chunk_size])\n",
        "\n",
        "chunks = []\n",
        "for txt in doc_texts:\n",
        "    for c in chunk_text(txt, chunk_size=400):\n",
        "        chunks.append(c)\n",
        "\n",
        "len(chunks), chunks[:2]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "2FHvTZhwVO3P",
        "outputId": "60690148-bcba-45cd-e9ed-6c037872462e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1,\n",
              " ['This is a sample document for testing the RAG system. It contains some text about Retrieval Augmented Generation.'])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create embeddings and FAISS index"
      ],
      "metadata": {
        "id": "XeJHM9P0VSU_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import faiss\n",
        "\n",
        "# compute embeddings\n",
        "chunk_embeddings = embed_model.encode(chunks, show_progress_bar=True, convert_to_numpy=True)\n",
        "\n",
        "# build FAISS index (L2)\n",
        "dim = chunk_embeddings.shape[1]\n",
        "index = faiss.IndexFlatIP(dim)   # use Inner Product if embeddings normalized; otherwise IndexFlatL2\n",
        "# Normalize for IP:\n",
        "faiss.normalize_L2(chunk_embeddings)\n",
        "\n",
        "index.add(chunk_embeddings)\n",
        "\n",
        "# keep mapping back\n",
        "metadata = [{\"doc_id\": i, \"text\": chunks[i]} for i in range(len(chunks))]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "9eac5ea59a544010b676d5727b8d12f7",
            "4d5110dd64274f5ba1c21fc1b410e2ed",
            "42c6611d18fd47a1afa9d49e28e67ddc",
            "a3590a943958495ebdcd27e96ee817a0",
            "4f626d206f2f4b9da99bedb5a29a34c2",
            "3ae0d12039bf4d48b38e4f4cb19be02e",
            "9fc67eb556604676990e0885ecf56eae",
            "5bbc13059dbd4f539d8e102ff21425c9",
            "740e4029d3944f75a0d2d8e8c9d32b46",
            "4ba9d1c540bc44d3ab7d30d952ee0028",
            "c8b3addc6a3e4c1297da028bb954966d"
          ]
        },
        "id": "0U1Hvk0WVUyv",
        "outputId": "24e088f4-7be3-4b6d-a8d1-860abca25be4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9eac5ea59a544010b676d5727b8d12f7"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01fde676"
      },
      "source": [
        "# Create a dummy docs directory and add a sample file\n",
        "import os\n",
        "\n",
        "if not os.path.exists(\"docs\"):\n",
        "    os.makedirs(\"docs\")\n",
        "\n",
        "with open(\"docs/sample_doc.txt\", \"w\") as f:\n",
        "    f.write(\"This is a sample document for testing the RAG system. It contains some text about Retrieval Augmented Generation.\")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retrieval helper"
      ],
      "metadata": {
        "id": "J8dhlcWSWDgo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve(query, top_k=4):\n",
        "    q_emb = embed_model.encode([query], convert_to_numpy=True)\n",
        "    faiss.normalize_L2(q_emb)\n",
        "    D, I = index.search(q_emb, top_k)\n",
        "    results = [metadata[i] for i in I[0]]\n",
        "    return results\n",
        "\n",
        "# test\n",
        "print(retrieve(\"How does the gesture-to-text system handle real-time inference?\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "whjnI7hTWDPK",
        "outputId": "55301cf2-498f-4281-c987-78b84ca4fc51"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'doc_id': 0, 'text': 'This is a sample document for testing the RAG system. It contains some text about Retrieval Augmented Generation.'}, {'doc_id': 0, 'text': 'This is a sample document for testing the RAG system. It contains some text about Retrieval Augmented Generation.'}, {'doc_id': 0, 'text': 'This is a sample document for testing the RAG system. It contains some text about Retrieval Augmented Generation.'}, {'doc_id': 0, 'text': 'This is a sample document for testing the RAG system. It contains some text about Retrieval Augmented Generation.'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "— Build prompt from retrieved chunks + user query"
      ],
      "metadata": {
        "id": "F_ul5ZL9W2pz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RAG_PROMPT_TEMPLATE = \"\"\"You are an assistant. Use the context excerpts below (they are verbatim chunks from domain docs). If the answer is not in the provided context, say \"I don't know\" or provide a best effort with disclaimers.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "User question:\n",
        "{question}\n",
        "\n",
        "Answer concisely and cite which chunk index you used when applicable.\n",
        "\"\"\"\n",
        "\n",
        "def rag_answer(question, top_k=4, max_new_tokens=256):\n",
        "    hits = retrieve(question, top_k=top_k)\n",
        "    context = \"\\n\\n\".join([f\"[chunk {h['doc_id']}]\\n{h['text']}\" for h in hits])\n",
        "    prompt = RAG_PROMPT_TEMPLATE.format(context=context, question=question)\n",
        "    return generate_from_model(prompt, max_new_tokens=max_new_tokens)\n",
        "\n",
        "# Example\n",
        "print(rag_answer(\"What preprocessing pipeline was used for PSI gesture landmarks?\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Nmj-WjnBW6aq",
        "outputId": "84881cae-6114-466b-baef-0399eb809452"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are an assistant. Use the context excerpts below (they are verbatim chunks from domain docs). If the answer is not in the provided context, say \"I don't know\" or provide a best effort with disclaimers.\n",
            "\n",
            "Context:\n",
            "[chunk 0]\n",
            "This is a sample document for testing the RAG system. It contains some text about Retrieval Augmented Generation.\n",
            "\n",
            "[chunk 0]\n",
            "This is a sample document for testing the RAG system. It contains some text about Retrieval Augmented Generation.\n",
            "\n",
            "[chunk 0]\n",
            "This is a sample document for testing the RAG system. It contains some text about Retrieval Augmented Generation.\n",
            "\n",
            "[chunk 0]\n",
            "This is a sample document for testing the RAG system. It contains some text about Retrieval Augmented Generation.\n",
            "\n",
            "User question:\n",
            "What preprocessing pipeline was used for PSI gesture landmarks?\n",
            "\n",
            "Answer concisely and cite which chunk index you used when applicable.\n",
            "I don't know.  I was unable to find any information about a preprocessing pipeline for PSI gesture landmarks in the provided context.  I can try to help find more information if you have more context or details.  Please provide more information about PSI gesture landmarks or the relevant context.  I'll do my best to assist.  I don't have any information about PSI gesture landmarks in the provided context.  I can try to help you find the answer if you provide more context or details.  Please let me know how I can assist further.  I don't have any information about PSI gesture landmarks in the provided context.  I can try to help you find the answer if you provide more context or details.  Please let me know how I can assist further.  I don't have any information about PSI gesture landmarks in the provided context.  I can try to help you find the answer if you provide more context or details.  Please let me know how I can assist further.  I don't have any information about PSI gesture landmarks in the provided context.  I can try to help you find the answer if you provide more context or details.  Please let me know how I can assist further.  I don't have any information about PSI gesture\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flask flask-cors pyngrok\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "pbn1iVPJX3fk",
        "outputId": "f6d78598-ec5d-42c2-d19e-adc1bf77603b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: flask in /usr/local/lib/python3.12/dist-packages (3.1.2)\n",
            "Requirement already satisfied: flask-cors in /usr/local/lib/python3.12/dist-packages (6.0.1)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.12/dist-packages (7.4.0)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from flask) (1.9.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.12/dist-packages (from flask) (8.2.1)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from flask) (2.2.0)\n",
            "Requirement already satisfied: jinja2>=3.1.2 in /usr/local/lib/python3.12/dist-packages (from flask) (3.1.6)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from flask) (3.0.2)\n",
            "Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from flask) (3.1.3)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.12/dist-packages (from pyngrok) (6.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CREATING API FOR INTEGRATING IN SYSTEM"
      ],
      "metadata": {
        "id": "zDm5V5HpX6Rk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, request, jsonify\n",
        "from flask_cors import CORS\n",
        "from pyngrok import ngrok\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Create Flask app\n",
        "app = Flask(__name__)\n",
        "CORS(app)\n",
        "\n",
        "@app.route(\"/rag\", methods=[\"POST\"])\n",
        "def rag_api():\n",
        "    data = request.get_json()\n",
        "    question = data.get(\"question\", \"\")\n",
        "    answer = rag_answer(question)  # uses your function from Cell 11\n",
        "    return jsonify({\"answer\": answer})\n",
        "\n",
        "# Set the ngrok authtoken from Colab secrets\n",
        "NGROK_AUTH_TOKEN = userdata.get('NGROK_AUTH_TOKEN')\n",
        "if NGROK_AUTH_TOKEN:\n",
        "    ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "    print(\"ngrok authtoken set.\")\n",
        "else:\n",
        "    print(\"NGROK_AUTH_TOKEN not found in Colab secrets. Please add it.\")\n",
        "\n",
        "# Disconnect any existing ngrok tunnels\n",
        "ngrok.kill()\n",
        "\n",
        "# Open an ngrok tunnel on port 5000\n",
        "public_url = ngrok.connect(5000)\n",
        "print(\"🔥 Your Colab API URL is:\", public_url)\n",
        "\n",
        "# Run the Flask app\n",
        "app.run(port=5000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmY_yuKRX-It",
        "outputId": "8190c205-2a20-4033-de17-55909578b670"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [26/Sep/2025 19:17:04] \"POST /rag HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ngrok authtoken set.\n",
            "🔥 Your Colab API URL is: NgrokTunnel: \"https://kale-unlucky-mellisa.ngrok-free.dev\" -> \"http://localhost:5000\"\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "WARNING:pyngrok.process.ngrok:t=2025-09-26T19:17:11+0000 lvl=warn msg=\"failed to check for update\" obj=updater err=\"Post \\\"https://update.equinox.io/check\\\": context deadline exceeded\"\n",
            "INFO:werkzeug:127.0.0.1 - - [26/Sep/2025 19:17:13] \"OPTIONS /rag HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [26/Sep/2025 19:17:46] \"OPTIONS /rag HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [26/Sep/2025 19:18:11] \"POST /rag HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [26/Sep/2025 19:18:12] \"POST /rag HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [26/Sep/2025 19:18:12] \"POST /rag HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [26/Sep/2025 19:18:12] \"POST /rag HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [26/Sep/2025 19:18:13] \"POST /rag HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [26/Sep/2025 19:18:30] \"POST /rag HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [26/Sep/2025 19:18:35] \"POST /rag HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [26/Sep/2025 19:19:06] \"POST /rag HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [26/Sep/2025 19:19:08] \"POST /rag HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [26/Sep/2025 19:19:47] \"OPTIONS /rag HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [26/Sep/2025 19:20:11] \"POST /rag HTTP/1.1\" 200 -\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "res = requests.post(\"https://kale-unlucky-mellisa.ngrok-free.dev/rag\",\n",
        "                    json={\"question\": \"What is Retrieval Augmented Generation?\"})\n",
        "print(res.json())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "_SE-FrMIcH7u",
        "outputId": "70d118ed-e220-46c0-e32c-0ec49bf79e18"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "JSONDecodeError",
          "evalue": "Expecting value: line 1 column 1 (char 0)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mjson\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    975\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 976\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcomplexjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    977\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mJSONDecodeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/simplejson/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, use_decimal, allow_nan, **kw)\u001b[0m\n\u001b[1;32m    513\u001b[0m             and not use_decimal and not allow_nan and not kw):\n\u001b[0;32m--> 514\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    515\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/simplejson/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w, _PY3)\u001b[0m\n\u001b[1;32m    385\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/simplejson/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx, _w, _PY3)\u001b[0m\n\u001b[1;32m    415\u001b[0m                 \u001b[0midx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2880190736.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m res = requests.post(\"https://kale-unlucky-mellisa.ngrok-free.dev/rag\", \n\u001b[1;32m      4\u001b[0m                     json={\"question\": \"What is Retrieval Augmented Generation?\"})\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mjson\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    978\u001b[0m             \u001b[0;31m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m             \u001b[0;31m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 980\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRequestsJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    981\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    982\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
          ]
        }
      ]
    }
  ]
}